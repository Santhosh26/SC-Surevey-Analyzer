"""
Appendix Section
"""

from datetime import datetime
from pptx import Presentation
from pptx.util import Inches, Pt
from fpdf import FPDF

from .base import BaseSection


class AppendixSection(BaseSection):
    """
    Generates Appendix slide with methodology and data sources.
    """

    def get_slide_count(self) -> int:
        return 1

    def generate_pptx_slides(self, prs: Presentation) -> Presentation:
        slide = self.add_content_slide(prs, "Appendix: Methodology & Data Sources")

        # Methodology section
        self.add_text_box(
            slide, 0.5, 1.3, 6.0, 0.4,
            "Methodology", font_size=14, bold=True, color='primary'
        )

        methodology = [
            "Survey conducted via Mentimeter during International All-Hands",
            "Open-ended responses analyzed using contextual sentiment engine",
            "8-rule scoring system with question bias awareness",
            "AI summaries generated using Claude Opus 4.5 via AWS Bedrock",
            "Hallucination validation performed on all LLM outputs",
        ]

        y_pos = 1.8
        for item in methodology:
            self.add_text_box(
                slide, 0.5, y_pos, 6.0, 0.4,
                f"  {item}", font_size=10, color='text'
            )
            y_pos += 0.45

        # Data sources section
        self.add_text_box(
            slide, 7.0, 1.3, 5.5, 0.4,
            "Data Sources", font_size=14, bold=True, color='primary'
        )

        # Get stats
        raw_data = self.data.get('raw_data')
        llm_data = self.data.get('llm_summaries', {})

        total_responses = len(raw_data) if raw_data is not None else 'N/A'
        total_questions = len(raw_data['Question'].unique()) if raw_data is not None else 'N/A'
        llm_generated = llm_data.get('metadata', {}).get('generated_at', 'N/A')

        sources = [
            f"Total survey responses: {total_responses}",
            f"Questions analyzed: {total_questions}",
            f"AI summaries generated: {llm_generated}",
            f"Presentation created: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        ]

        y_pos = 1.8
        for item in sources:
            self.add_text_box(
                slide, 7.0, y_pos, 5.5, 0.4,
                f"  {item}", font_size=10, color='text'
            )
            y_pos += 0.45

        # Confidence note
        self.add_text_box(
            slide, 0.5, 5.5, 12.0, 1.0,
            "Note: Sentiment confidence scores range from 0.5-1.0. Results with confidence < 0.7 "
            "should be reviewed manually. AI-generated summaries have been validated for factual accuracy "
            "but may contain interpretation that differs from raw data.",
            font_size=9, color='neutral'
        )

        # Footer
        self.add_text_box(
            slide, 0.5, 6.7, 12.0, 0.4,
            "Generated by Presales Survey Analysis Toolkit  |  claude.ai/code",
            font_size=10, color='secondary', align='center'
        )

        return prs

    def generate_pdf_content(self, pdf: FPDF) -> FPDF:
        self.pdf_add_content_page(pdf, "Appendix: Methodology & Data Sources")

        # Methodology
        pdf.set_font('Helvetica', 'B', 12)
        pdf.set_text_color(*self.pdf_colors['primary'])
        pdf.cell(0, 8, "Methodology", new_x='LMARGIN', new_y='NEXT')

        methodology = [
            "Survey conducted via Mentimeter during International All-Hands",
            "Open-ended responses analyzed using contextual sentiment engine",
            "8-rule scoring system with question bias awareness",
            "AI summaries generated using Claude Opus 4.5 via AWS Bedrock",
            "Hallucination validation performed on all LLM outputs",
        ]

        pdf.set_font('Helvetica', '', 10)
        pdf.set_text_color(*self.pdf_colors['text'])
        for item in methodology:
            pdf.cell(5, 6, chr(149))
            pdf.cell(0, 6, item, new_x='LMARGIN', new_y='NEXT')

        pdf.ln(5)

        # Data sources
        raw_data = self.data.get('raw_data')
        llm_data = self.data.get('llm_summaries', {})

        total_responses = len(raw_data) if raw_data is not None else 'N/A'
        total_questions = len(raw_data['Question'].unique()) if raw_data is not None else 'N/A'
        llm_generated = llm_data.get('metadata', {}).get('generated_at', 'N/A')

        pdf.set_font('Helvetica', 'B', 12)
        pdf.set_text_color(*self.pdf_colors['primary'])
        pdf.cell(0, 8, "Data Sources", new_x='LMARGIN', new_y='NEXT')

        pdf.set_font('Helvetica', '', 10)
        pdf.set_text_color(*self.pdf_colors['text'])
        pdf.cell(0, 6, f"Total survey responses: {total_responses}", new_x='LMARGIN', new_y='NEXT')
        pdf.cell(0, 6, f"Questions analyzed: {total_questions}", new_x='LMARGIN', new_y='NEXT')
        pdf.cell(0, 6, f"AI summaries generated: {llm_generated}", new_x='LMARGIN', new_y='NEXT')
        pdf.cell(0, 6, f"Presentation created: {datetime.now().strftime('%Y-%m-%d %H:%M')}", new_x='LMARGIN', new_y='NEXT')

        pdf.ln(10)

        # Note
        pdf.set_font('Helvetica', 'I', 9)
        pdf.set_text_color(*self.pdf_colors['neutral'])
        pdf.multi_cell(0, 5,
            "Note: Sentiment confidence scores range from 0.5-1.0. Results with confidence < 0.7 "
            "should be reviewed manually. AI-generated summaries have been validated for factual accuracy "
            "but may contain interpretation that differs from raw data."
        )

        pdf.ln(10)
        pdf.set_font('Helvetica', '', 10)
        pdf.set_text_color(*self.pdf_colors['secondary'])
        pdf.cell(0, 8, "Generated by Presales Survey Analysis Toolkit  |  claude.ai/code", align='C')

        return pdf
